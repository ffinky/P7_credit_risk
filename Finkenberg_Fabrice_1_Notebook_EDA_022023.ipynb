{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P7 Credit Risk (EDA pour prise en main des data)\n",
    "**Présentation du projet**\n",
    "- Sujet : aide à la décision concernant l'octroi de crédit \n",
    "- Objectif : prédire les personnes qui risquent d'être en défaut de paiement en fonction de leur comportement dans le passé\n",
    "- Data : historique des donnéées comportementales sur les crédits qui précèdent la demande actuelle des clients \n",
    "- Problématique : classification supervisée qui retourne un score qui mesure la probabilité que le client appartienne à la classe possitive (client risqué)\n",
    "\n",
    "**Mise en oeuvre**\n",
    "- Versionning sous Git/Github\n",
    "- Data processing (data cleaning, feature engineering)\n",
    "- Modélisation via LightGBM (kernel récupéré de Kaggle)\n",
    "- Evaluation (AUC, courbe ROC...)\n",
    "- Automatisation des traitements\n",
    "- Analyses exploratoires et identification des 20 features les + importantes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-built general packages\n",
    "import gc # Garbage Collector pour supprimer de l'utilisation mémoire\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data imputing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Data encoding (variables catégorielles)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Classifier\n",
    "# lightgbm est 1 bibliothèque de Microsoft qui fournit 1 algo d'amplification du gradien + efficace + rapide que les autres bibliothèques\n",
    "from lightgbm import LGBMClassifier # algo d'amplification du gradient (Gradient Boosting Machine) ou XGBoost/CatBoost basés sur des tree\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score # évaluation des modèles de classification\n",
    "from sklearn.model_selection import KFold, StratifiedKFold # stratégies de classification\n",
    "\n",
    "# Warnings management\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. EDA - Application train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de application train : (307511, 122)\n"
     ]
    }
   ],
   "source": [
    "# Lecture des data application : demandes de crédits\n",
    "df = pd.read_csv('../input/application_train.csv', nrows= None)\n",
    "print(f'Dimensions de application train : {df.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la target \n",
    "df_target = df['TARGET'].value_counts().reset_index()\n",
    "df_target['TARGET_NAME'] = df['TARGET'].map({0:'No default', 1:'Default'})\n",
    "plt.pie(df_target['TARGET'], labels=df_target['TARGET_NAME'], autopct='%1.2f%%', pctdistance=0.6)\n",
    "plt.title('Distribution des crédits par TARGET')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,10))\n",
    "sns.heatmap(df.isna(), cbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des variables pour lesquelles les données manquantes représentent moins de 10% des échantillons toute classe confondue \n",
    "df_missing_10 = pd.DataFrame(df.isna().mean()*100, columns=['missing_%'])\n",
    "df_missing_10 = df_missing_10[(df_missing_10['missing_%']<10) & (df_missing_10['missing_%']>0)]\n",
    "print(f'Variables avec données manquantes < 10% :\\n{df_missing_10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des variables pour lesquelles les données manquantes représentent moins de 10% des échantillons sur la classe minoritaire (défaut de paiement) \n",
    "df_missing_10 = pd.DataFrame(df[df['TARGET']==1].isna().mean()*100, columns=['missing_%'])\n",
    "df_missing_10 = df_missing_10[(df_missing_10['missing_%']<10) & (df_missing_10['missing_%']>0)]\n",
    "print(f'Variables avec données manquantes < 10% sur la classe minoritaire:\\n{df_missing_10}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des variables pour lesquelles les données manquantes représentent moins de 10% des échantillons sur la classe majoritaire (pas de défaut de paiement) \n",
    "df_missing_10_majo = pd.DataFrame(df[df['TARGET']==0].isna().mean()*100, columns=['missing_%'])\n",
    "df_missing_10_majo = df_missing_10_majo[(df_missing_10_majo['missing_%']<10) & (df_missing_10_majo['missing_%']>0)]\n",
    "print(f'Variables avec données manquantes < 10% sur la classe majoritaire :\\n{df_missing_10_majo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau des valeurs manquantes\n",
    "freq = df.isna().sum() \n",
    "avg = round(df.isna().mean()*100,2)\n",
    "df_missing = pd.DataFrame()\n",
    "df_missing['col'] = freq.index\n",
    "df_missing['missing'] = freq.values\n",
    "df_missing['missing_freq_%'] = avg.values\n",
    "df_missing = df_missing.sort_values(by='missing', ascending=False)\n",
    "print(f\"Nb de colonnes avec des data manquantes : {df_missing[df_missing['missing']>0].shape[0]}\")\n",
    "display(df_missing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.4. Imputing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des types de data\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement des variables quantitatives manquantes\n",
    "col_qty_missing = pd.DataFrame(df.select_dtypes('float64').isna().sum().sort_values(), columns=['missing'])\n",
    "col_qty_missing = list(col_qty_missing[col_qty_missing['missing']>0].index)\n",
    "print(f'Liste des var quantitatives avec données manquantes {len(col_qty_missing)} : \\n{col_qty_missing}')\n",
    "\n",
    "# Imputation des Nan par la médiane insensible aux valeurs extrêmes\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "df[col_qty_missing] = imp_median.fit_transform(df[col_qty_missing])\n",
    "\n",
    "# Résultat après imputation \n",
    "print(f\"\\nAprès imputation : {df.select_dtypes('float64').isna().sum().sum()} valeurs quantitatives manquantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement des variables catégorielles manquantes\n",
    "col_cat_missing = pd.DataFrame(df.select_dtypes('object').isna().sum().sort_values(), columns=['missing'])\n",
    "print(col_cat_missing)\n",
    "col_cat_missing = list(col_cat_missing[col_cat_missing['missing']>0].index)\n",
    "print(f'\\nListe des var catégorielles avec données manquantes : \\n{col_cat_missing}')\n",
    "\n",
    "# Imputation des Nan par la valeur catégorielle la + fréquente\n",
    "imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "df[col_cat_missing] = imp_most_frequent.fit_transform(df[col_cat_missing])\n",
    "\n",
    "# Résultat après imputation \n",
    "print(f\"\\nAprès imputation : {df.select_dtypes('object').isna().sum().sum()} valeurs catégorielles manquantes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.5. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement des outliers\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier : DAYS_BIRTH < 0\n",
    "(df['DAYS_BIRTH']/-365).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de l'âge\n",
    "sns.displot(df['DAYS_BIRTH']/-365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ajoute AGE + TR_AGE \n",
    "df['AGE'] = df['DAYS_BIRTH']/-365\n",
    "df['TR_AGE'] = pd.cut(df['AGE'], bins=np.linspace(20, 70, num=6)) # split en 6 tranches d'âge de 10 ans\n",
    "df[['AGE', 'TR_AGE', 'DAYS_BIRTH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier : DAYS_EMPLOYED\n",
    "sns.displot(df['DAYS_EMPLOYED'], kind='kde') # distribution\n",
    "\n",
    "# max(DAYS_EMPLOYED) = 365243 > 1000 années, or ne peut pas être > 70 ans (25500 jours)\n",
    "print(f\"Nbre de lignes Tq DAYS_EMPLOYED > 70 ans {df[df['DAYS_EMPLOYED']>25500].shape[0]}\")\n",
    "print(f\"Liste des valeurs > 70 ans : {df[df['DAYS_EMPLOYED']>25500]['DAYS_EMPLOYED'].unique()}\")\n",
    "\n",
    "# Imputation de la valeur extrême par Nan (ou par la médiane/moyenne ?)\n",
    "df.loc[df['DAYS_EMPLOYED']>25500, 'DAYS_EMPLOYED'] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.6. Features correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des catégories de type FLAG censées contenir que 2 valeurs\n",
    "for col in ['CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY']:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des data catégorielles incohérentes (cencées contenir que 2 valeurs)\n",
    "df = df[df['CODE_GENDER'] != 'XNA']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.7. Features encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoding pour les varaiables catégorielles à 2 valeurs (flag : 0/1)\n",
    "lab_encoder = LabelEncoder()\n",
    "col_cat = list(df.select_dtypes('object').columns)\n",
    "cnt = 0\n",
    "print('Label encodage pour les variables catégorielles suivantes à 2 valeurs :')\n",
    "for col in col_cat :\n",
    "    if df[col].nunique() <= 2 :\n",
    "        cnt+=1\n",
    "        df[col] = lab_encoder.fit_transform(df[col])\n",
    "        print(f'- {col}')\n",
    "print(f'Nombre de variables encodées avec Label Encoding : {cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding pour les variables catégorielles à plus de 2 valeurs\n",
    "col_origine = list(df.columns)\n",
    "col_cat = list(df.select_dtypes('object').columns)\n",
    "\n",
    "# Encodage One-hot des variables catégorielles (dans df on ne conserve pas les var catégorielles d'origine)\n",
    "df = pd.get_dummies(df, columns=col_cat, dummy_na=False)\n",
    "\n",
    "# Nouveaux libellés des colonnes encodées : \"columnName_value\"\n",
    "new_col = [c for c in list(df.columns) if c not in col_origine]\n",
    "print(f'{len(new_col)} Nouvelles colonnes one hot encodées :\\n{new_col}')\n",
    "df[new_col].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.8. Exploratory analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des défauts de paiement (TARGET) par tranches d'âge\n",
    "df_tr_age = df[['TR_AGE', 'AGE', 'TARGET']].copy()\n",
    "target_groupby_tr_age = df_tr_age.groupby('TR_AGE').mean().reset_index()\n",
    "print(target_groupby_tr_age)\n",
    "\n",
    "# barplot\n",
    "df_tr_age = df[['TR_AGE', 'AGE', 'TARGET']].copy()\n",
    "target_groupby_tr_age = df_tr_age.groupby('TR_AGE').mean().reset_index()\n",
    "sns.barplot(data=target_groupby_tr_age, x='TR_AGE', y='TARGET')\n",
    "plt.title('Défaut de paiement par tranche d\\'âge')\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation des features avec la target\n",
    "cor = df.corr()['TARGET'].sort_values(ascending=False)\n",
    "print(f'Top 20 :\\n{cor.head(20)}')\n",
    "print(f'\\nBottom 20 :\\n{cor.tail(20)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data pre-processing \n",
    "- Fonctions d'automatisation des traitements de préparation des données \n",
    "- Généralisation de ce qui a été fait dans le chapitre **I. EDA** pour l'ensemble des tables/fichiers csv\n",
    "- Ces fonctions sont **exécutées en chaîne** dans le chapitre **III.2 Main()** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# General functions\n",
    "####################\n",
    "# Suppression des individus SSI le nombre de valeurs manquante <= 10% de l'échantillon d'une classe donnée\n",
    "# valable pour les variables quantitatives et catégorielles\n",
    "def fct_dropna(df):\n",
    "    dict_classes = {0:'majoritaire', \n",
    "                    1:'minoritaire'\n",
    "    }\n",
    "    list_df_classe = [] # liste pour contenir les 2 dataframes : df découpé par classe \n",
    "    \n",
    "    # Suppression des échantillons de valeurs manquantes par classe\n",
    "    for classe, val in dict_classes.items(): \n",
    "        # Echantilons par classe \n",
    "        list_df_classe.append(None)\n",
    "        df_classe = df[df['TARGET']==classe].copy()\n",
    "        taille_classe = df_classe.shape[0]\n",
    "\n",
    "        # Identification des variables pour lesquelles le nbre de valeurs manquantes est <= 10% de la taille de classe\n",
    "        df_missing_10 = pd.DataFrame(df[df['TARGET']==classe].isna().sum(), columns=['missing'])\n",
    "        df_missing_10['missing_%'] = round(100*df_missing_10['missing']/taille_classe, 2)\n",
    "        df_missing_10 = df_missing_10[(df_missing_10['missing_%']<=10) & (df_missing_10['missing_%']>0)]\n",
    "        df_missing_10 = df_missing_10.sort_values(by='missing_%', ascending=False) \n",
    "\n",
    "        # Liste des variables\n",
    "        cols = df_missing_10.index\n",
    "        print(f'Classe {classe} {val}')\n",
    "        print('-'*50) \n",
    "        \n",
    "        # dropna par classe\n",
    "        nb_to_be_deleted = df_missing_10['missing'].sum() \n",
    "        if nb_to_be_deleted > 0:\n",
    "            print(f\"NaN à supprimer : {nb_to_be_deleted}\")\n",
    "            print(f'Liste des NaN à supprimer Tq données manquantes <= 10% :\\n{df_missing_10}')\n",
    "            df_classe.dropna(axis=0, subset=cols, inplace=True)\n",
    "            print(f'\\nNombre d\\'échantillon avant suppression NaN : {taille_classe}, après suppression NaN : {df_classe.shape[0]}')\n",
    "            print(f'Nombre d\\'échantillon supprimés : {taille_classe-df_classe.shape[0]} ({(taille_classe-df_classe.shape[0])/taille_classe:.2f}%)')\n",
    "        else:\n",
    "            print('Pas de NaN à supprimer !')\n",
    "        # save le df par classe dans 1 liste\n",
    "        list_df_classe[classe] = df_classe.copy()\n",
    "        print('')\n",
    "\n",
    "    # Dataframe résultat de la concaténation des dropna par classes \n",
    "    df = pd.concat([list_df_classe[0], list_df_classe[1]], axis=0, ignore_index=True)\n",
    "\n",
    "    # Retourne les données après dropna\n",
    "    return df\n",
    "\n",
    "# Imputation des variables quantitatives NaN par la médiane\n",
    "# Imputation des variables catétégorielles NaN par most_frequent \n",
    "# SSI le nombre de valeurs manquantes > 10% de l'échantillon d'une classe donnée \n",
    "def fct_fillna(df):\n",
    "    dict_classes = {0:'majoritaire', \n",
    "                    1:'minoritaire'\n",
    "    }\n",
    "    ###################################################\n",
    "    # Imputation des variables quantitatives par classe\n",
    "    ###################################################\n",
    "    df2 = df.copy()\n",
    "    df2 = df2[['TARGET'] + list(df.select_dtypes('float64').columns)]\n",
    "    imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    for classe, val in dict_classes.items():\n",
    "        taille_classe = df2[df2['TARGET']==classe].shape[0] # taille échantillon par classe\n",
    "        df_missing_10 = pd.DataFrame(df2[df2['TARGET']==classe].isna().sum(), columns=['missing'])\n",
    "        df_missing_10['missing_%'] = round(100*df_missing_10['missing']/taille_classe, 2)\n",
    "        df_missing_10 = df_missing_10[df_missing_10['missing_%']>10]\n",
    "        df_missing_10 = df_missing_10.sort_values(by='missing_%', ascending=False)\n",
    "\n",
    "        # Liste des variables quantitatives à imputer\n",
    "        cols = df_missing_10.index\n",
    "        print(f'Classe {classe} {val} (var quantitatives)')\n",
    "        print('-'*50)\n",
    "\n",
    "        # Imputation quantitative par classes\n",
    "        nb_to_be_imputed = df_missing_10['missing'].sum() # nombre d'échantillon NaN à imputer\n",
    "        if nb_to_be_imputed > 0:\n",
    "            print(f\"NaN à imputer : {nb_to_be_imputed}\")\n",
    "            print(f'Détail des NaN à imputer (par la médiane) Tq données manquantes > 10% :\\n{df_missing_10}')\n",
    "            df.loc[df['TARGET']==classe, cols] = imp_median.fit_transform(df[df['TARGET']==classe][cols])\n",
    "            nb_nan = df[df['TARGET']==classe][cols].isna().sum().sum() # NaN restant\n",
    "            print(f\"\\nNaN après imputation : {nb_nan}, NaN imputés : {nb_to_be_imputed-nb_nan}\")\n",
    "        else:\n",
    "            print('Pas de NaN à imputer !')\n",
    "        print('')\n",
    "\n",
    "    ###################################################\n",
    "    # Imputation des variables catégorielles par classe\n",
    "    ###################################################\n",
    "    df2 = df.copy()\n",
    "    df2 = df2[['TARGET'] + list(df2.select_dtypes('object').columns)]\n",
    "    imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    \n",
    "    for classe, val in dict_classes.items():\n",
    "        taille_classe = df2[df2['TARGET']==classe].shape[0] # taille échantillon par classe\n",
    "        df_missing_10 = pd.DataFrame(df2[df2['TARGET']==classe].isna().sum(), columns=['missing'])\n",
    "        df_missing_10['missing_%'] = round(100*df_missing_10['missing']/taille_classe, 2) \n",
    "        df_missing_10 = df_missing_10[df_missing_10['missing_%']>10]\n",
    "        df_missing_10 = df_missing_10.sort_values(by='missing_%', ascending=False)\n",
    "        \n",
    "        # Liste des colonnes catégorielles à imputer\n",
    "        cols = df_missing_10.index\n",
    "        print(f'Classe {classe} {val} (var catégorielles)')\n",
    "        print('-'*50)\n",
    "        \n",
    "        # Imputation catégorielle par classes   \n",
    "        nb_to_be_imputed = df_missing_10['missing'].sum() # nombre d'échantillon NaN à imputer\n",
    "        if nb_to_be_imputed > 0:\n",
    "            print(f\"NaN à imputer : {nb_to_be_imputed}\")\n",
    "            print(f'Détail des NaN à imputer (par most frequent) Tq données manquantes > 10% :\\n{df_missing_10}')\n",
    "            df.loc[df['TARGET']==classe, cols] = imp_most_frequent.fit_transform(df[df['TARGET']==classe][cols])\n",
    "            nb_nan = df[df['TARGET']==classe][cols].isna().sum().sum() # NaN restant\n",
    "            print(f\"NaN après imputation : {nb_nan}, NaN imputés : {nb_to_be_imputed-nb_nan}\")\n",
    "        else:\n",
    "            print('Pas de NaN à imputer !')\n",
    "        print('')\n",
    "\n",
    "    # Retourne les données après imputation\n",
    "    return df\n",
    "\n",
    "# Fonction générale de tratement des Nan par classe(TARGET) au niveau de Application \n",
    "def fct_dropna_fillna(df):\n",
    "    df = fct_dropna(df) # suppression NaN si valeurs manquantes <= 10% de l'échantillon par classe\n",
    "    df = fct_fillna(df) # imputation NaN par la médiane si valeurs manquantes > 10% de l'échantillon par classe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des NaN par 0 pour les colonnes aggrégats sans correspondance dans la table gauche jointe (df)\n",
    "# Utilisé dans le preprocessing des fichiers/tables après chaque jointure gauche effectuée.\n",
    "# A distinguer du traitement général des NaN, effectué au niveau du fichier/table Application\n",
    "\"\"\"\n",
    "Paramètres :\n",
    "- df : dataframe qui correspond à la table cible de la jointure gauche des aggrégats\n",
    "- cols_agg : liste des colonnes aggrégats\n",
    "- title : nom de l'aggrégation effectuée\n",
    "Output:\n",
    "- df : dataframe dont les valeurs NaN des colonnes aggrégées ont été remplacées\n",
    "       par 0 pour indiquer que l'indicateur n'existe pas.\n",
    "\"\"\"\n",
    "def fct_fillna_agg(df, cols_agg, title):\n",
    "    print(f\"\\nAggrégat {title} - nombre de NaN (suite aux left join) à remplacer par 0 :\\n{(df[cols_agg].isna().sum()).sort_values(ascending=False)}\\n\")\n",
    "    df[cols_agg] = df[cols_agg].fillna(value=0, axis=0)\n",
    "    print(f\"Nombre de NaN restant : {df[cols_agg].isna().sum().sum()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# General functions (suite)\n",
    "###########################\n",
    "# Pour le calcul de la durée des traitements (s'utilise avec \"with\")\n",
    "@contextmanager\n",
    "def fct_timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"{title} - done in {time.time() - t0:.0f}\")\n",
    "\n",
    "# Label encoding (encodage binaire) pour les var catégorielles avec seulement 2 valeurs (flag : 0/1)\n",
    "def fct_label_encoder(df, title):\n",
    "    lab_encoder = LabelEncoder()\n",
    "    col_cat = list(df.select_dtypes('object').columns)\n",
    "    cnt = 0\n",
    "    for col in col_cat :\n",
    "        if df[col].nunique() <= 2 :\n",
    "            cnt+=1\n",
    "            print(f'Table {title} - Label encodage pour la variable binaire : {col}')\n",
    "            df[col] = lab_encoder.fit_transform(df[col])\n",
    "    print(f'Table {title} - nombre de variables binaires Label encodées : {cnt}')\n",
    "    # Retourne le df binaire encodé\n",
    "    return df\n",
    "\n",
    "# One-hot encoding pour les var catégorielles à plus de 2 valeurs (get_dummies)\n",
    "def fct_one_hot_encoder(df, nan_as_category=True, title=''):\n",
    "    col_origine = list(df.columns)\n",
    "    col_cat = list(df.select_dtypes('object').columns)\n",
    "    # Encodage One-hot des variables catégorielles (dans df on ne conserve pas les var catégorielles d'origine)\n",
    "    df = pd.get_dummies(df, columns=col_cat, dummy_na=nan_as_category)\n",
    "    # Nouveaux libellés des colonnes encodées : \"columnName_value\"\n",
    "    new_col = [c for c in df.columns if c not in col_origine]\n",
    "    if len(new_col)>0:\n",
    "        print(f'\\nTable {title} - nombre de variables One Hot encodées : {len(new_col)}')\n",
    "        print(new_col)\n",
    "    else:\n",
    "        print(f'\\nTable {title} - nombre de variables One Hot encodées : 0')\n",
    "    # Retourne le df one hot encodé avec la liste des nouvelles colonnes encodées\n",
    "    return df, new_col\n",
    "\n",
    "# Nettoyage des valeurs NaN et des noms de colonnes contenant des caractères spéciaux\n",
    "# Fill NaN --> 0 : correspond aux valeurs sans correspondance dans les jointures gauche (left join)\n",
    "def fct_clean_data(df):\n",
    "    # Fill NaN par 0\n",
    "    feat = [v for v in list(df.columns) if v != 'TARGET']\n",
    "    df[feat] = df[feat].fillna(0, axis=0)\n",
    "\n",
    "    # Bug : \"[LightGBM] Do not support special JSON characters in feature name\"\n",
    "    # Correction : suppression des caractères spéciaux ' ', ','... dans le nom des colonnes\n",
    "    import re\n",
    "    new_names = {col: re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns}\n",
    "    new_names_list = list(new_names.values())\n",
    "    \n",
    "    # Nom des colonnes unique : ajout du suffix i si le nom de la colonne apparaît plus d'1 fois après suppression des caractères spéciaux\n",
    "    new_names = {col: f'{new_col}_{i}' if new_col in new_names_list[:i] else new_col for i, (col, new_col) in enumerate(new_names.items())}\n",
    "    df.columns = new_names.values()\n",
    "\n",
    "    # Retourne le dataframe nettoyé\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1. Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOME CREDIT DEFAULT RISK\n",
    "\"\"\" Most features are created by applying min, max, mean, sum and var functions to grouped tables. \n",
    "Little feature selection is done and overfitting might be a problem since many features are related.\n",
    "The following key ideas were used:\n",
    "- Divide or subtract important features to get rates (like annuity and income)\n",
    "- In Bureau Data: create specific features for Active credits and Closed credits\n",
    "- In Previous Applications: create specific features for Approved and Refused applications\n",
    "- Modularity: one function for each table (except bureau_balance and application_test)\n",
    "- One-hot encoding for categorical features\n",
    "All tables are joined with the application DF using the SK_ID_CURR key (except bureau_balance).\n",
    "You can use LightGBM with KFold or Stratified KFold.\n",
    "\"\"\"\n",
    "# Preprocess : application_train.csv + application_test.csv\n",
    "def fct_application_train_test(num_rows=None, nan_as_category=True, process_nan=True):\n",
    "    # Lecture des data application train/test et merge\n",
    "    df = pd.read_csv('../input/application_train.csv', nrows=num_rows, encoding='utf8')\n",
    "    test_df = pd.read_csv('../input/application_test.csv', nrows=num_rows, encoding='utf8')\n",
    "    print(f'Echantillon TRAIN : {len(df)}, Echantillon TEST : {len(test_df)}\\n')\n",
    "\n",
    "    # Nettoyage : suppression de 4 lignes en erreur Tq CODE_GENDER = 'XNA' (TRAIN set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "    # Outliers : NaN values for DAYS_EMPLOYED > 25500 days = 70 years (1 value found = 365243 days)\n",
    "    #df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    df.loc[df['DAYS_EMPLOYED']>25500, 'DAYS_EMPLOYED'] = np.nan\n",
    "\n",
    "    # Traitement des NaN par classes (TARGET) : dropna si <= 10% des échantillons, imputation par la médiane si > 10% des échantillons \n",
    "    if process_nan:\n",
    "        df = fct_dropna_fillna(df)\n",
    "\n",
    "    # Concaténation Train + Test set\n",
    "    df = df.append(test_df).reset_index()\n",
    "    df.drop(columns='index', inplace=True)\n",
    "    \n",
    "    # Encodage binaire (0/1) pour les catégories ayant que 2 valeurs possibles\n",
    "    #for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "    #    df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    df = fct_label_encoder(df, title='application')\n",
    "\n",
    "    # Encodage One-Hot pour les catégories avec plus de 2 valeurs possibles\n",
    "    df, cat_cols = fct_one_hot_encoder(df, nan_as_category, title='application')\n",
    "    \n",
    "    # Feature engineering : exemples de création de nouvelles variables (ratio...)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH'] # jours employés / âge en jours\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT'] # revenu_total / montant du crédit\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS'] # revenu_total / nombre de membres composant la famille\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL'] # montant remb annuel / revenu total\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT'] # montant remb annuel / montant du crédit \n",
    "    df['TR_AGE'] = pd.cut(df['DAYS_BIRTH']/-365, bins=[20,30,40,50,60,70], labels=False) # Tranche âge : split en 5 tranches de 10 ans\n",
    "    \n",
    "    del test_df # suppr de l'objet devenu inutile\n",
    "    gc.collect() # vide la RAM\n",
    "    \n",
    "    # Retourne le dataframe concaténé TRAIN + TEST set, nettoyé et encodé \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appli = fct_application_train_test(num_rows=None, nan_as_category=True, process_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Check des data manquantes de application\n",
    "df_appli_miss = pd.DataFrame(df.isna().mean()*100, columns=['%_miss_value'])\n",
    "df_appli_miss = df_appli_miss[df_appli_miss['%_miss_value']>0].sort_values(by='%_miss_value', ascending=False)\n",
    "# Barplot des data manquantes\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "sns.set()\n",
    "sns.barplot(y=df_appli_miss.index, x=df_appli_miss['%_miss_value'])\n",
    "plt.yticks(fontsize=7)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2. Bureau et Bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def fct_bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    # Lecture des fichiers bureau et bureau_balance\n",
    "    df_bureau = pd.read_csv('../input/bureau.csv', nrows=num_rows, encoding='utf8')\n",
    "    df_bb = pd.read_csv('../input/bureau_balance.csv', nrows=num_rows, encoding='utf8')\n",
    "    print(f'Echantillon Bureau : {len(df_bureau)}')\n",
    "    print(f'Echantillon Bureau_Balance : {len(df_bb)}\\n')\n",
    "\n",
    "    # Traitement des NaN / classe \n",
    "    #df_bureau = fct_dropna_fillna(df_bureau)\n",
    "    #df_bb = fct_dropna_fillna(df_bb)\n",
    "\n",
    "    # Encodage binaire (0/1) pour les catégories ayant que 2 valeurs possibles\n",
    "    df_bureau = fct_label_encoder(df_bureau, title='bureau')\n",
    "    df_bb = fct_label_encoder(df_bb, title='bureau_balance')\n",
    "\n",
    "    # Encodage One hot des variables catégorielles\n",
    "    df_bureau, bureau_cat = fct_one_hot_encoder(df_bureau, nan_as_category, title='bureau')\n",
    "    df_bb, bb_cat = fct_one_hot_encoder(df_bb, nan_as_category, title='bureau_balance')\n",
    "\n",
    "    ##########################################################\n",
    "    # Step 1 : aggregations par bureau + merge avec bureau.csv\n",
    "    # Bureau balance : features quantitatives et catégorielles\n",
    "    ########################################################## \n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        # Ajout la (clé, val) = (catégorie bb, mean) dans le dictionnaire d'aggrégation\n",
    "        # obj : calculer la moyenne des crédits par statut 0,1..5,C,X (col) \n",
    "        bb_aggregations[col] = ['mean'] \n",
    "    \n",
    "    # Applique toutes les aggrégations bb sur la clé bureau en 1 fois\n",
    "    bb_agg = df_bb.groupby('SK_ID_BUREAU').agg(bb_aggregations) \n",
    "    \n",
    "    # Transforme les colonnes MultiIndex [(col1, agg1), (col1, agg2),...] en colonnes MonoIndex : col1_agg1, col1_agg2...\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    \n",
    "    # Ajout des aggrégations bb dans df_bureau\n",
    "    df_bureau = df_bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    df_bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True) # suppr de la clé devenue inutile\n",
    "    # Remplace les valeurs NaN des colonnes agrégées (indicateurs) par 0 pour indiquer que les indicateurs n'existent pas \n",
    "    #df_bureau = fct_fillna_agg(df_bureau, bb_agg.columns.tolist(), title='bureau balance')\n",
    "    \n",
    "    # Suppr des objects devenus inutiles et libère la RAM\n",
    "    del df_bb, bb_agg  \n",
    "    gc.collect()\n",
    "    \n",
    "    ###################################################\n",
    "    # Step 2 : aggrégations par application\n",
    "    # Bureau et bureau_balance : features quantitatives\n",
    "    ###################################################\n",
    "    num_aggregations = {\n",
    "        # bureau : aggregation par application des indicateurs de bureau\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        # bureau balance : aggregation par application des aggrégats bb par bureau\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau et bureau_balance : features catégorielles\n",
    "    cat_aggregations = {} # dictionnaire\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    #{**num_aggregations, **cat_aggregations} = concaténation des 2 dictionnaires\n",
    "    # pour chaque indicateur numérique on calcul les aggrégations num : min, max...\n",
    "    # pour chaque catégorie encodée one hot on calcul la moyenne\n",
    "    bureau_agg = df_bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    \n",
    "    # Bureau: Active credits - aggregations sur les features quantitatives\n",
    "    active = df_bureau[df_bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR') # ajout de Active credits aggrégats dans bureau_agg\n",
    "    # Remplace les valeurs NaN des colonnes agrégées (indicateurs) par 0 pour indiquer que les indicateurs n'existent pas \n",
    "    # bureau_agg = fct_fillna_agg(bureau_agg, active_agg.columns.tolist(), title='active_credit')\n",
    "\n",
    "    # Suppr des objets devenus inutiles et libère la RAM\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau: Closed credits - aggregations sur les features quantitatives\n",
    "    closed = df_bureau[df_bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR') # ajout de Closed credits aggrégats dans bureau_agg\n",
    "    # Remplace les valeurs NaN des colonnes agrégées (indicateurs) par 0 pour indiquer que les indicateurs n'existent pas \n",
    "    # bureau_agg = fct_fillna_agg(bureau_agg, closed_agg.columns.tolist(), title='closed_credit')\n",
    "    \n",
    "    # Suppr des objets devenus inutiles et libère la RAM\n",
    "    del closed, closed_agg, df_bureau \n",
    "    gc.collect()\n",
    "\n",
    "    # Retourne les données agrégées par application dans bureau_agg\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = fct_bureau_and_balance(num_rows=None)\n",
    "bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs manquantes\n",
    "bureau.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3. Previous application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def fct_previous_applications(num_rows=None, nan_as_category=True):\n",
    "    # Lecture du fichier des applications précédentes et encodage des catégories\n",
    "    df_prev = pd.read_csv('../input/previous_application.csv', nrows=num_rows, encoding='utf8')\n",
    "    print(f'Echantillon Previous Application : {len(df_prev)}\\n')\n",
    "    \n",
    "    # Outliers : Days=365243 --> nan\n",
    "    df_prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    df_prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    df_prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    df_prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    df_prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # Traitement des NaN\n",
    "    #df_prev = fct_dropna_fillna(df_prev)\n",
    "    \n",
    "    # Encodage binaire + one hot \n",
    "    df_prev = fct_label_encoder(df_prev, title='previous_application')\n",
    "    df_prev, cat_cols = fct_one_hot_encoder(df_prev, nan_as_category, title='previous_application')\n",
    "    \n",
    "    # Feature engineering : add feature --> montant de crédit demandé / montant reçu en %\n",
    "    df_prev['APP_CREDIT_PERC'] = df_prev['AMT_APPLICATION'] / df_prev['AMT_CREDIT']\n",
    "    \n",
    "    # Aggrégations : features quantitatives de previous applications\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    \n",
    "    # Aggregations : features catégorielles de prévious application\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    # Calcul des aggregations par application\n",
    "    prev_agg = df_prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    \n",
    "    # Calcul des aggregations \"Approved Applications\" - sur les features quantitatives\n",
    "    approved = df_prev[df_prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Remplace les valeurs NaN des colonnes agrégées (indicateurs) par 0 pour indiquer que les indicateurs n'existent pas \n",
    "    # prev_agg = fct_fillna_agg(prev_agg, approved_agg.columns.tolist(), title='application_approved')\n",
    "    \n",
    "    # Calcul des aggregations \"Refused Applications\" - sur les features quantitatives\n",
    "    refused = df_prev[df_prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    # Remplace les valeurs NaN des colonnes agrégées (indicateurs) par 0 pour indiquer que les indicateurs n'existent pas \n",
    "    # prev_agg = fct_fillna_agg(prev_agg, refused_agg.columns.tolist(), title='application_refused')\n",
    "    \n",
    "    # Gestion de la RAM\n",
    "    del refused, refused_agg, approved, approved_agg, df_prev\n",
    "    gc.collect()    \n",
    "    \n",
    "    # Retourne les données aggrégées de previous application par application\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev_agg = fct_previous_applications(num_rows = None, nan_as_category=True)\n",
    "df_prev_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4. Pos cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess POS_CASH_balance.csv\n",
    "def fct_pos_cash(num_rows=None, nan_as_category=True):\n",
    "    # Lecture du fichier pos cash\n",
    "    df_pos = pd.read_csv('../input/POS_CASH_balance.csv', nrows=num_rows, encoding='utf8')\n",
    "    print(f'Echantillon Pos Cash : {len(df_pos)}\\n')\n",
    "    \n",
    "    # Traitement des NaN\n",
    "    #df_pos = fct_dropna_fillna(df_pos)\n",
    "\n",
    "    # Encodage binaire + one hot \n",
    "    df_pos = fct_label_encoder(df_pos, title='pos cash')\n",
    "    df_pos, cat_cols = fct_one_hot_encoder(df_pos, nan_as_category, title='pos cash')\n",
    "    \n",
    "    # Aggrégation popur les variables quantitatives et catégorielles\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    # Aggregation du cash par application\n",
    "    pos_agg = df_pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    \n",
    "    # Comptage du nombre de comptes pos cash\n",
    "    pos_agg['POS_COUNT'] = df_pos.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    # Libère la RAM\n",
    "    del df_pos\n",
    "    gc.collect()\n",
    "    \n",
    "    # Retourne les données agrégées pos cash par application\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_agg = fct_pos_cash(num_rows = None, nan_as_category = True)\n",
    "df_pos_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.5. Installments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "def fct_installments_payments(num_rows=None, nan_as_category=True):\n",
    "    # Lecture du fichier des versements\n",
    "    df_ins = pd.read_csv('../input/installments_payments.csv', nrows=num_rows, encoding='utf8')\n",
    "    print(f'Echantillon Installments : {len(df_ins)}\\n')\n",
    "    \n",
    "    # Traitement des NaN\n",
    "    #df_ins = fct_dropna_fillna(df_ins)\n",
    "\n",
    "    # Encodage binaire + one hot\n",
    "    df_ins = fct_label_encoder(df_ins, title='installments')\n",
    "    df_ins, cat_cols = fct_one_hot_encoder(df_ins, nan_as_category, title='installments')\n",
    "    \n",
    "    # Feature engineering : % + difference de montant de paiement pour chaque versement\n",
    "    df_ins['PAYMENT_PERC'] = df_ins['AMT_PAYMENT'] / df_ins['AMT_INSTALMENT']\n",
    "    df_ins['PAYMENT_DIFF'] = df_ins['AMT_INSTALMENT'] - df_ins['AMT_PAYMENT']\n",
    "    \n",
    "    # Feature engineering : DPD = Days Past Due; days before due (no negative values)\n",
    "    df_ins['DPD'] = df_ins['DAYS_ENTRY_PAYMENT'] - df_ins['DAYS_INSTALMENT']\n",
    "    df_ins['DBD'] = df_ins['DAYS_INSTALMENT'] - df_ins['DAYS_ENTRY_PAYMENT']\n",
    "    df_ins['DPD'] = df_ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    df_ins['DBD'] = df_ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # Préparartion des aggrégations des var quantitatives et catégorielles\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    # Aggregation par application\n",
    "    ins_agg = df_ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    \n",
    "    # Comptage du nombre de comptes de versement\n",
    "    ins_agg['INSTAL_COUNT'] = df_ins.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    # Libère la RAM\n",
    "    del df_ins\n",
    "    gc.collect()\n",
    "    \n",
    "    # Retourne les données aggrégées des versements par application\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ins_agg = fct_installments_payments(num_rows = None, nan_as_category = True)\n",
    "df_ins_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.6. Credit card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess credit_card_balance.csv\n",
    "def fct_credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    # Lecture du fichier des soldes cartes de crédit\n",
    "    df_cc = pd.read_csv('../input/credit_card_balance.csv', nrows=num_rows, encoding='utf8')\n",
    "    print(f'Echantillon credit card : {len(df_cc)}\\n')\n",
    "    \n",
    "    # Traitement des NaN\n",
    "    #df_cc = fct_dropna_fillna(df_cc)\n",
    "    \n",
    "    # Encodage binaire + one hot\n",
    "    df_cc = fct_label_encoder(df_cc, title='credit card')\n",
    "    df_cc, cat_cols = fct_one_hot_encoder(df_cc, nan_as_category, title='credit card')\n",
    "    \n",
    "    # Aggregations générales\n",
    "    df_cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = df_cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    \n",
    "    # Comptage du nombre de carte de credit\n",
    "    cc_agg['CC_COUNT'] = df_cc.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    # Libère la RAM\n",
    "    del df_cc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Retourne les données aggrégées de carte de crédit par application\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc_agg = fct_credit_card_balance(num_rows = None, nan_as_category = True)\n",
    "df_cc_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Features importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1. Gradient Boosting LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot des features importance, par défaut on choisit les 20 premières features\n",
    "def fct_display_importances(feature_importance_df_, importance_file_name_, top_=20):\n",
    "    # Calcul de la moyenne de l'importance par feature pour l'ensemble des folders\n",
    "    top_feat = feature_importance_df_[['feature', 'importance']].groupby('feature').mean().sort_values(by='importance', ascending=False)[:top_].index\n",
    "    df_best_feat = feature_importance_df_.loc[feature_importance_df_['feature'].isin(top_feat)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x='importance', y='feature', data=df_best_feat.sort_values(by='importance', ascending=False))\n",
    "    plt.title('LightGBM - features importance (moy sur les folders)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(importance_file_name_)\n",
    "    # Retourne  la liste du top 20 des features\n",
    "    return list(top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT avec stratégie KFold ou Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def fct_kfold_lightgbm(df, \\\n",
    "                       num_folds, \\\n",
    "                       stratified=False, \\\n",
    "                       debug=False, \\\n",
    "                       submission_file_name='submission.csv', \\\n",
    "                       importance_file_name='importance.png' \\\n",
    "                       ):\n",
    "    \n",
    "    # Split en TRAIN/VALIDATION set and TEST set\n",
    "    df_train = df[df['TARGET'].notnull()]\n",
    "    df_test = df[df['TARGET'].isnull()]\n",
    "    print(f'Starting LightGBM. train shape: {df_train.shape}, test shape: {df_test.shape}')\n",
    "    \n",
    "    # Libère la RAM\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    \n",
    "    # Numpy arrays et dataframes pour stocker le résultat des prédictions\n",
    "    oof_preds = np.zeros(df_train.shape[0])\n",
    "    sub_preds = np.zeros(df_test.shape[0])\n",
    "    df_feat_importance = pd.DataFrame()\n",
    "    \n",
    "    # Supprression des features X non pertinentes\n",
    "    feats = [f for f in df_train.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    # Split du TRAIN set en TRAIN/VALIDATION set (n folders) \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_train[feats], df_train['TARGET'])):\n",
    "        train_x, train_y = df_train[feats].iloc[train_idx], df_train['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = df_train[feats].iloc[valid_idx], df_train['TARGET'].iloc[valid_idx]\n",
    "        # Paramètres LightGBM trouvés par optimisation \"Bayesian\"\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1, # not used\n",
    "            verbose=-1, # not used\n",
    "        )\n",
    "        \n",
    "        # Entraînement du classifier sur la partie du TRAIN set correspondant au folder (split)\n",
    "        print(f'Fold {n_fold+1} ...')\n",
    "        clf.fit(\n",
    "            train_x, \n",
    "            train_y, \n",
    "            eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric='auc', \n",
    "            verbose=200, \n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "        \n",
    "        # Predictions sur la VALIDATION set correspondant au folder (split)\n",
    "        print('Validation prediction...')\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        # Prediction sur le TEST set\n",
    "        print('Test prediction...')\n",
    "        sub_preds += clf.predict_proba(df_test[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "        \n",
    "        # Features importance\n",
    "        print('Features importance...')\n",
    "        df_fold_importance = pd.DataFrame()\n",
    "        df_fold_importance['feature'] = feats\n",
    "        df_fold_importance['importance'] = clf.feature_importances_\n",
    "        df_fold_importance['fold'] = n_fold + 1\n",
    "        df_feat_importance = pd.concat([df_feat_importance, df_fold_importance], axis=0)\n",
    "        \n",
    "        # Evaluation de la performance sur la VALIDATION set : score ROC_AUC (Aire sous la courve ROC)\n",
    "        print('Validation AUC score...')\n",
    "        print(f'Fold {n_fold+1} AUC score : {roc_auc_score(valid_y, oof_preds[valid_idx]):.6f}')\n",
    "        \n",
    "        # Libère la RAM\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # Evaluation de la performance globale sur le TRAIN set\n",
    "    print(f\"Full AUC score {roc_auc_score(df_train['TARGET'], oof_preds):.6f}\")\n",
    "    \n",
    "    # En mode Run, sauvegarde dans fichier \"submission_file_name\"\n",
    "    if not debug:\n",
    "        # Prédiction du futur sur le test set : y TARGET = probabilité de prédiction \n",
    "        df_test['TARGET'] = sub_preds\n",
    "        df_test[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index=False)\n",
    "    \n",
    "    # Plot des features importance. On renvoit la liste du top 20 des features importance\n",
    "    top_feat = fct_display_importances(df_feat_importance, importance_file_name)\n",
    "\n",
    "    # Data pour la courbe ROC\n",
    "    folds_idx = [(train_idx, valid_idx) \n",
    "                  for train_idx, valid_idx in folds.split(df_train[feats], df_train['TARGET'])]\n",
    "    y = df_train['TARGET']\n",
    "    \n",
    "    # Retourne le dataframe des top features importance + les éléments pour pouvoir afficher la courbe ROC \n",
    "    return top_feat, df_feat_importance, y, oof_preds, folds_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2. ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracé des courbes ROC\n",
    "\"\"\" Ce sont les couples (x=1-spécifité, y=sensibilité) en fonction du seuil de classe au dessus duquel les points sont prédits > 0\n",
    "principe : \n",
    "- on calcul la matrice de confusion pour chaque seuil (du seuil max = y max prédit... au seuil min = y min prédit)\n",
    "- on extrait la sensibilité et la spécificité de chaque matrice de confusion\n",
    "- on trace la courbe ROC : couples (x=1-spécifité, y=sensibilité)\n",
    "- on calcule l'AUC : Aire sous la courbe ROC qui doit être le + proche possible de 1 et > 0.5 (classification aléatoire))\n",
    "Parameters :\n",
    "- y_ : y true (étiquette 0/1)\n",
    "- oof_preds_ : y prediction de probailité (score entre 0 et 1)\n",
    "- folds_idx_ : intervalle des données \n",
    "Remarques :\n",
    "- ici pas de validation croisée : fit réalisé sur l'ensemble du TRAIN set  \n",
    "\"\"\" \n",
    "def fct_display_roc_curve(y_, oof_preds_, folds_idx_, roc_curve_file_name_):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # ROC + AUC sur la VALIDATION set\n",
    "    scores = []\n",
    "    # On parcours chaque folder du TRAIN set\n",
    "    for n_fold, (_, val_idx) in enumerate(folds_idx_):\n",
    "        # Eléments du tracé de la courbe ROC du VALIDATION set pour le folder : \n",
    "        # fpr = false positive rate (1-spécificité)\n",
    "        # tpr = true positive rate (sensibilité/rappel)\n",
    "        # thr = threshold (seuil de classification binaire)\n",
    "        fpr, tpr, thr = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n",
    "        \n",
    "        # Score AUC : évaluation de la performance du modèle sur le VALIDATION set  \n",
    "        score = roc_auc_score(y_.iloc[val_idx], oof_preds_[val_idx])\n",
    "        scores.append(score)\n",
    "        \n",
    "        # Courbe ROC du VALIDATION set (par folder)\n",
    "        plt.plot(\n",
    "            fpr,\n",
    "            tpr,\n",
    "            lw=1,\n",
    "            alpha=0.3,\n",
    "            label=f'ROC fold {n_fold+1} (AUC = {score:0.4f})' \n",
    "        )\n",
    "    \n",
    "    # ROC du modèle (baseline) de classification aléatoire\n",
    "    plt.plot(\n",
    "        [0, 1], [0, 1],\n",
    "        linestyle='--',\n",
    "        lw=2,\n",
    "        color='r',\n",
    "        label='Random classifier',\n",
    "        alpha=.8\n",
    "    )\n",
    "    \n",
    "    # ROC + AUC sur le TRAIN set \n",
    "    fpr, tpr, thr = roc_curve(y_, oof_preds_)\n",
    "    score = roc_auc_score(y_, oof_preds_)\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color='b',\n",
    "        label=f'Avg ROC (AUC = {score:0.4f} $\\pm$ {np.std(scores):0.4f})', # moy ROC +/- l'écart type\n",
    "        lw=2,\n",
    "        alpha=.8\n",
    "    )\n",
    "\n",
    "    # Paramètres graphiques\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1-spécificité)')\n",
    "    plt.ylabel('True Positive Rate (sensibilité/rappel)')\n",
    "    plt.title('LightGBM ROC Curve')\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    # Sauvegarde de l'image graphique\n",
    "    plt.savefig(roc_curve_file_name_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.3. AUC-PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracé de la courbe AUC-PR\n",
    "def fct_display_precision_recall(y_, oof_preds_, folds_idx_, pr_curve_file_name_):\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    scores = []\n",
    "    for n_fold, (_, val_idx) in enumerate(folds_idx_):\n",
    "        # Plot the roc curve\n",
    "        fpr, tpr, thr = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n",
    "        score = average_precision_score(y_.iloc[val_idx], oof_preds_[val_idx])\n",
    "        scores.append(score)\n",
    "        plt.plot(\n",
    "            fpr,\n",
    "            tpr,\n",
    "            lw=1,\n",
    "            alpha=0.3,\n",
    "            label='AP fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n",
    "\n",
    "    precision, recall, thr = precision_recall_curve(y_, oof_preds_)\n",
    "    score = average_precision_score(y_, oof_preds_)\n",
    "    plt.plot(\n",
    "        precision,\n",
    "        recall,\n",
    "        color='b',\n",
    "        label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n",
    "        lw=2,\n",
    "        alpha=.8)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('LightGBM Recall / Precision')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(pr_curve_file_name_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Automation\n",
    "Ici on teste 2 scenarii de traitement des NaN et on automatise les traitements :\n",
    "- data pre-processing + feature engineering\n",
    "- affichage des features importance avec LightGBM\n",
    "- affichage des courbes ROC + PR \n",
    "- plusieurs sauvegardes (20 features les + importantes, tous les features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top20_feat(debug=False, \\\n",
    "                      nan_as_category=False, \\\n",
    "                      process_nan=False, \\\n",
    "                      submission_file_name='submission.csv', \\\n",
    "                      importance_file_name='importances.png', \\\n",
    "                      roc_curve_file_name='roc_curve.png', \\\n",
    "                      pr_curve_file_name='pr_curve.png'):\n",
    "    \n",
    "    num_rows = 10000 if debug else None\n",
    "    df = fct_application_train_test(num_rows, nan_as_category=nan_as_category, process_nan=process_nan)\n",
    "    with fct_timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = fct_bureau_and_balance(num_rows)\n",
    "        print(\"\\nBureau df shape :\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        # df = fct_fillna_agg(df, bureau.columns.tolist(), title='left join bureau to application')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with fct_timer(\"Process previous_applications\"):\n",
    "        prev = fct_previous_applications(num_rows, nan_as_category=nan_as_category)\n",
    "        print(\"\\nPrevious applications df shape :\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with fct_timer(\"Process POS-CASH balance\"):\n",
    "        pos = fct_pos_cash(num_rows, nan_as_category=False)\n",
    "        print(\"\\nPos-cash balance df shape: \", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with fct_timer(\"Process installments payments\"):\n",
    "        ins = fct_installments_payments(num_rows, nan_as_category=nan_as_category)\n",
    "        print(\"\\nInstallments payments df shape: \", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with fct_timer(\"Process credit card balance\"):\n",
    "        cc = fct_credit_card_balance(num_rows, nan_as_category=nan_as_category)\n",
    "        print(\"\\nCredit card balance df shape : \", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    with fct_timer(\"Data & columns cleaning\"):\n",
    "        df = fct_clean_data(df)\n",
    "    with fct_timer(\"Run LightGBM with 5 kfold\"):\n",
    "        top_feat, df_feat_importance, y, oof_preds, folds_idx = fct_kfold_lightgbm(df, \n",
    "                                                                                   num_folds=5, \n",
    "                                                                                   stratified=False, \n",
    "                                                                                   debug=debug, \n",
    "                                                                                   submission_file_name=submission_file_name,\n",
    "                                                                                   importance_file_name=importance_file_name)\n",
    "    with fct_timer(\"ROC curve\"):\n",
    "        fct_display_roc_curve(y, oof_preds, folds_idx, roc_curve_file_name)\n",
    "    with fct_timer(\"PR curve\"):\n",
    "        fct_display_precision_recall(y, oof_preds, folds_idx, pr_curve_file_name)\n",
    "\n",
    "    return top_feat, df\n",
    "    #return top_feat, df, df_feat_importance, y, oof_preds, folds_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Main()\n",
    "\"\"\" Execution de la procédure principale d'éxtraction des 20 features les + importants pour la TARGET \n",
    "selon 2 scenarii : \n",
    "1) process_nan = False : pas de traitement particulier des NaN (NaN --> 0)\n",
    "2) process_nan = True : \n",
    "                        - traitement particulier des NaN/TARGET pour Application (dropna si < 10%, impute = median/most_frequent si > 10%)   \n",
    "                        - pour les NaN restants issus des autres fichiers --> 0\n",
    "\"\"\" \n",
    "def fct_main(nan_as_category=False):\n",
    "    for scenario in range(2):\n",
    "        process_nan = True if scenario==1 else False\n",
    "        submission_file_name = \"submission_kernel_\" + str(scenario) + \".csv\"\n",
    "        importance_file_name = \"lgbm_importances_\" + str(scenario) + \".png\"\n",
    "        roc_curve_file_name = \"roc_curve_\" + str(scenario) + \".png\"\n",
    "        pr_curve_file_name = \"precision_recall_curve_\" + str(scenario) + \".png\"\n",
    "        with fct_timer(\"Full model LightGBM run\"):\n",
    "            # Retourne le dataset df et les features importance\n",
    "            print(f'Itération {scenario} - process_nan (Application) = {process_nan} ...')\n",
    "            top_feat, df = select_top20_feat(debug=False, \n",
    "                                             nan_as_category=True, \n",
    "                                             process_nan=process_nan, \n",
    "                                             submission_file_name=submission_file_name,\n",
    "                                             importance_file_name=importance_file_name,\n",
    "                                             roc_curve_file_name=roc_curve_file_name,\n",
    "                                             pr_curve_file_name=pr_curve_file_name)\n",
    "            df.to_csv('df_scenario_' + str(scenario) + '.csv')\n",
    "            df_feat20 = df[['TARGET'] + top_feat] \n",
    "            df_feat20.to_csv('df_feat20_scenario_' + str(scenario) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécution ALL\n",
    "fct_main(nan_as_category=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.1. Features analysis\n",
    "Top 20 des features les + importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture du fichier scenario 0\n",
    "#df_1 = df.save() # scenario 1\n",
    "df = pd.read_csv('df_feat20_scenario_0.csv')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "print(f\"Check NaN :\\n{(df.isna().mean()*100).sort_values(ascending=False)}\")\n",
    "feat20 = df.columns.tolist()\n",
    "print('-'*50)\n",
    "print(f'Top 20 features :\\n{feat20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de distribution type barplot pour la variable catégorielle \"col\"\n",
    "def fct_bar(df, col):\n",
    "    data = pd.DataFrame(df[col].value_counts(), columns=([col]))\n",
    "    fig = plt.figure(figsize=(8,10))\n",
    "    sns.barplot(data=data, y=data.index, x=col)\n",
    "    plt.title('Distribution de {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de distribution type histogramme pour la variable quantitative \"col\"\n",
    "def fct_hist(df, col):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    #sns.displot(data=df, x=col, kind='kde', hue='TARGET')\n",
    "    sns.kdeplot(data=df[df['TARGET']==0], x=col, label='Pas en défaut', color='g')\n",
    "    sns.kdeplot(data=df[df['TARGET']==1], x=col, label='En défaut', color='r')\n",
    "    plt.title(f'Distribution de {col}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des features top 20\n",
    "for feat in [col for col in feat20 if col != 'TARGET']:\n",
    "    fct_hist(df, feat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0d12d0507485905f91de5a3ae1496085db90787cb30f3d2c798edd152ae7949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
